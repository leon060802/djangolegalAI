{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NCU\\FirstSemester\\LegalAI\\django\\django-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å…¥æ–‡ç« : äº”ã€å¦‚ä¸æœæœ¬è£å®šï¼Œæ‡‰æ–¼è£å®šé€é”å¾Œ10æ—¥å…§ï¼Œä»¥æ›¸ç‹€å‘æœ¬é™¢å¸æ³•äº‹å‹™å®˜ææ˜¥ç•°è­°ã€‚æœ¬æ¡ˆä¾æ³•åˆ¤æ±ºï¼Œåˆè­°åº­å¯©ç†å®Œç•¢ã€‚\n",
      "ä¿®æ­£å¾Œæ–‡ç« : ä¸‰ã€å¦‚ä¸æœæœ¬è£å®šï¼Œæ‡‰æ–¼è£å®šé€é”å¾Œ10æ—¥å…§ï¼Œä»¥æ›¸ç‹€å‘æœ¬é™¢å¸æ³•äº‹å‹™å®˜æå‡ºç•°è­°ã€‚æœ¬æ¡ˆä¾æ³•åˆ¤æ±ºï¼Œåˆè­°åº­å¯©ç†å®Œç•¢ã€‚\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from run_relm import PTuningWrapper\n",
    "\n",
    "def load_model_and_tokenizer(model_path, pretrained_model_path, prompt_length=1):\n",
    "    \"\"\"è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)\n",
    "    base_model = BertForMaskedLM.from_pretrained(pretrained_model_path)\n",
    "    model = PTuningWrapper(base_model, prompt_length)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\"), weights_only=True))\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def predict_sentence(sentence, tokenizer, model, prompt_length):\n",
    "    \"\"\"å°å–®å¥é€²è¡Œæ¨¡å‹æ¨è«–èˆ‡ä¿®æ­£\"\"\"\n",
    "    # Tokenize sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=128, padding=\"max_length\", truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # ç”Ÿæˆ prompt_mask\n",
    "    prompt_mask = torch.zeros_like(input_ids)\n",
    "    prompt_mask[:, :2 * prompt_length] = 1\n",
    "\n",
    "    # æ¨¡å‹æ¨è«–\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=inputs[\"attention_mask\"], \n",
    "                        prompt_mask=prompt_mask,  \n",
    "                        apply_prompt=True)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # è§£ç¢¼ä¸¦æ¸…ç† token\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(predictions[0])\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    # æ¸…ç† token\n",
    "    return clean_tokens_with_numbers(input_tokens[1:], predicted_tokens[1:])\n",
    "\n",
    "def clean_tokens_with_numbers(input_tokens, predicted_tokens):\n",
    "    \"\"\"æ¸…ç†é æ¸¬çš„ tokensï¼Œä¿ç•™æ•¸å­—ä¸è®Šï¼Œè™•ç† ## æ‹¼æ¥\"\"\"\n",
    "    clean_text = \"\"\n",
    "    for input_token, predicted_token in zip(input_tokens, predicted_tokens):\n",
    "        if input_token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            continue\n",
    "        if input_token.isdigit() and predicted_token.isdigit() and input_token != predicted_token:\n",
    "            clean_text += input_token\n",
    "        elif predicted_token.startswith(\"##\"):\n",
    "            clean_text += predicted_token[2:]\n",
    "        else:\n",
    "            clean_text += predicted_token\n",
    "    return clean_text\n",
    "\n",
    "def process_article(article_text, tokenizer, model, prompt_length=1):\n",
    "    \"\"\"å°‡æ–‡ç« åˆ†å¥ä¸¦é€²è¡Œä¿®æ­£ï¼Œè¼¸å‡ºå®Œæ•´ä¿®æ­£å¾Œçš„æ–‡ç« \"\"\"\n",
    "    sentences = [s + \"ã€‚\" for s in article_text.split(\"ã€‚\") if s]  # åˆ†å¥ä¸¦ä¿ç•™å¥è™Ÿ\n",
    "    corrected_sentences = []\n",
    "    for sentence in sentences:\n",
    "        corrected_sentence = predict_sentence(sentence, tokenizer, model, prompt_length)\n",
    "        corrected_sentences.append(corrected_sentence)\n",
    "    return \"\".join(corrected_sentences)\n",
    "\n",
    "# === ä¸»ç¨‹å¼ ===\n",
    "if __name__ == \"__main__\":\n",
    "    # è¨­å®šè·¯å¾‘\n",
    "    model_path = \"D:/NCU/FirstSemester/LegalAI/relm_autocorrection/Judgement_Process_Artical/step-10200_f1-76.67.bin\"  # æ›¿æ›æˆä½ çš„æ¨¡å‹æ¬Šé‡è·¯å¾‘\n",
    "    pretrained_model_path = \"bert-base-chinese\"\n",
    "    prompt_length = 1\n",
    "\n",
    "    # è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer\n",
    "    tokenizer, model = load_model_and_tokenizer(model_path, pretrained_model_path, prompt_length)\n",
    "\n",
    "    # è¼¸å…¥æ–‡ç« å­—ä¸²\n",
    "    article_text = \"äº”ã€å¦‚ä¸æœæœ¬è£å®šï¼Œæ‡‰æ–¼è£å®šé€é”å¾Œ10æ—¥å…§ï¼Œä»¥æ›¸ç‹€å‘æœ¬é™¢å¸æ³•äº‹å‹™å®˜ææ˜¥ç•°è­°ã€‚æœ¬æ¡ˆä¾æ³•åˆ¤æ±ºï¼Œåˆè­°åº­å¯©ç†å®Œç•¢ã€‚\"\n",
    "    \n",
    "    # ä¿®æ­£æ–‡ç« \n",
    "    corrected_article = process_article(article_text, tokenizer, model, prompt_length)\n",
    "    \n",
    "    # è¼¸å‡ºçµæœ\n",
    "    print(\"è¼¸å…¥æ–‡ç« :\", article_text)\n",
    "    print(\"ä¿®æ­£å¾Œæ–‡ç« :\", corrected_article)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "django-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
